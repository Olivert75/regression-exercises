{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f374eb6e-e3ce-4a4e-a827-04d4e539d1fe",
   "metadata": {},
   "source": [
    "Exercises\n",
    "\n",
    "Do you work for this exercise in either a jupyter notebook named evaluate within your regression-exercises repository. By the end of this exercise you will also create a python script named evaluate, so the overall deliverables for this exercise are the python script and jupyter notebook both with the name evaluate within your regression-exercises repo.\n",
    "\n",
    "Load the tips dataset from either pydataset or seaborn.\n",
    "\n",
    "Fit a linear regression model (ordinary least squares) and compute yhat, predictions of tip using total_bill.\n",
    "\n",
    "Here is some sample code to get you started:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# assuming X and y are already defined\n",
    "model = LinearRegression().fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "Modify and add to the code above as necessary for it to work with the tips dataset.\n",
    "\n",
    "Plot the residuals for the linear regression model that you made.\n",
    "\n",
    "Calculate the sum of squared errors, explained sum of squares, total sum of squares, mean squared error, and root mean squared error for your model.\n",
    "\n",
    "Calculate the sum of squared errors, mean squared error, and root mean squared error for the baseline model (i.e. a model that always predicts the average tip amount).\n",
    "\n",
    "Write python code that compares the sum of squared errors for your model against the sum of squared errors for the baseline model and outputs whether or not your model performs better than the baseline model.\n",
    "\n",
    "What is the amount of variance explained in your model?\n",
    "\n",
    "Is your model better than the baseline model?\n",
    "\n",
    "Create a file named evaluate.py that contains the following functions.\n",
    "\n",
    "plot_residuals(y, yhat): creates a residual plot\n",
    "regression_errors(y, yhat): returns the following values:\n",
    "sum of squared errors (SSE)\n",
    "explained sum of squares (ESS)\n",
    "total sum of squares (TSS)\n",
    "mean squared error (MSE)\n",
    "root mean squared error (RMSE)\n",
    "baseline_mean_errors(y): computes the SSE, MSE, and RMSE for the baseline model\n",
    "better_than_baseline(y, yhat): returns true if your model performs better than the baseline, otherwise false\n",
    "Load the mpg dataset and fit a model that predicts highway mileage based on engine displacement. Take a look at all the regression evaluation metrics, and determine whether this model is better than the baseline model. Use the functions from your evaluate.py to help accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a5e6e3-284c-4e48-87a5-38c463c04b32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l_/4qjmp_nd08s2tfrcx49v2b980000gn/T/ipykernel_4608/2935499729.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplained_variance_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pydataset import data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, explained_variance_score\n",
    "from math import sqrt\n",
    "import evaluate\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9949a-ce08-4e2a-870d-9756ff9c299b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
